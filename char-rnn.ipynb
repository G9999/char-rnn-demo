{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6548ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 19:32:45.286065: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 19:32:46.010964: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-25 19:32:46.011488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-25 19:32:46.077599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:46.077826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 3060 Ti computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 38 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-11-25 19:32:46.077846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-25 19:32:46.080720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-11-25 19:32:46.080766: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-11-25 19:32:46.082036: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-25 19:32:46.082209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-25 19:32:46.084372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-25 19:32:46.084859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-11-25 19:32:46.084947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-25 19:32:46.085041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:46.085254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:46.085392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# set gpu devices if available\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    print(device)\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2162064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 858397 characters\n",
      "117 unique characters\n"
     ]
    }
   ],
   "source": [
    "text = open('dataset.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "# the unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac8d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 19:32:47.873189: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-11-25 19:32:47.873351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:47.873661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 3060 Ti computeCapability: 8.6\n",
      "coreClock: 1.695GHz coreCount: 38 deviceMemorySize: 7.77GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2021-11-25 19:32:47.873685: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-25 19:32:47.873701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-11-25 19:32:47.873707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-11-25 19:32:47.873714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-25 19:32:47.873720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-25 19:32:47.873725: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-25 19:32:47.873731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-11-25 19:32:47.873737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-25 19:32:47.873766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:47.873902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:47.874013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-11-25 19:32:47.874032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-25 19:32:48.201596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-25 19:32:48.201623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-11-25 19:32:48.201628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-11-25 19:32:48.201787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:48.201946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:48.202072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 19:32:48.202196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5761 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:09:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "# methods to converte between ids and chars\n",
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "# create ids\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9741f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sequence length\n",
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "# create proper dataset\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836ff28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((280, 100), (280, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 280\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b86e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "# create model\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd5db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize training\n",
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45930a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 19:32:56.086597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-11-25 19:32:56.117286: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3792695000 Hz\n",
      "2021-11-25 19:32:56.223347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-11-25 19:32:56.663646: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-11-25 19:32:56.667539: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 6s 145ms/step - loss: 3.5998\n",
      "Epoch 2/150\n",
      "30/30 [==============================] - 5s 136ms/step - loss: 2.7299\n",
      "Epoch 3/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 2.3675\n",
      "Epoch 4/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 2.2333\n",
      "Epoch 5/150\n",
      "30/30 [==============================] - 5s 138ms/step - loss: 2.1459\n",
      "Epoch 6/150\n",
      "30/30 [==============================] - 5s 145ms/step - loss: 2.0575\n",
      "Epoch 7/150\n",
      "30/30 [==============================] - 5s 158ms/step - loss: 1.9703\n",
      "Epoch 8/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 1.8934\n",
      "Epoch 9/150\n",
      "30/30 [==============================] - 5s 158ms/step - loss: 1.8214\n",
      "Epoch 10/150\n",
      "30/30 [==============================] - 5s 157ms/step - loss: 1.7524\n",
      "Epoch 11/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 1.6837\n",
      "Epoch 12/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.6177\n",
      "Epoch 13/150\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 1.5551\n",
      "Epoch 14/150\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 1.4967\n",
      "Epoch 15/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.4422\n",
      "Epoch 16/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.3925\n",
      "Epoch 17/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.3491\n",
      "Epoch 18/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.3102\n",
      "Epoch 19/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.2746\n",
      "Epoch 20/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.2429\n",
      "Epoch 21/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 1.2125\n",
      "Epoch 22/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.1873\n",
      "Epoch 23/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.1606\n",
      "Epoch 24/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.1366\n",
      "Epoch 25/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.1144\n",
      "Epoch 26/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.0914\n",
      "Epoch 27/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 1.0714\n",
      "Epoch 28/150\n",
      "30/30 [==============================] - 5s 150ms/step - loss: 1.0491\n",
      "Epoch 29/150\n",
      "30/30 [==============================] - 5s 155ms/step - loss: 1.0292\n",
      "Epoch 30/150\n",
      "30/30 [==============================] - 5s 156ms/step - loss: 1.0067\n",
      "Epoch 31/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.9869\n",
      "Epoch 32/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.9665\n",
      "Epoch 33/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.9450\n",
      "Epoch 34/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.9223\n",
      "Epoch 35/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.9015\n",
      "Epoch 36/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.8796\n",
      "Epoch 37/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.8562\n",
      "Epoch 38/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.8318\n",
      "Epoch 39/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.8102\n",
      "Epoch 40/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.7855\n",
      "Epoch 41/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.7585\n",
      "Epoch 42/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.7313\n",
      "Epoch 43/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.7060\n",
      "Epoch 44/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.6809\n",
      "Epoch 45/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.6530\n",
      "Epoch 46/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.6248\n",
      "Epoch 47/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.5988\n",
      "Epoch 48/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.5716\n",
      "Epoch 49/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.5406\n",
      "Epoch 50/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.5152\n",
      "Epoch 51/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.4877\n",
      "Epoch 52/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.4611\n",
      "Epoch 53/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.4371\n",
      "Epoch 54/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.4143\n",
      "Epoch 55/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.3892\n",
      "Epoch 56/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.3665\n",
      "Epoch 57/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.3465\n",
      "Epoch 58/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.3264\n",
      "Epoch 59/150\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 0.3078\n",
      "Epoch 60/150\n",
      "30/30 [==============================] - 5s 163ms/step - loss: 0.2916\n",
      "Epoch 61/150\n",
      "30/30 [==============================] - 6s 186ms/step - loss: 0.2764\n",
      "Epoch 62/150\n",
      "30/30 [==============================] - 6s 178ms/step - loss: 0.2613\n",
      "Epoch 63/150\n",
      "30/30 [==============================] - 6s 172ms/step - loss: 0.2467\n",
      "Epoch 64/150\n",
      "30/30 [==============================] - 6s 179ms/step - loss: 0.2323\n",
      "Epoch 65/150\n",
      "30/30 [==============================] - 6s 165ms/step - loss: 0.2205\n",
      "Epoch 66/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.2103\n",
      "Epoch 67/150\n",
      "30/30 [==============================] - 5s 139ms/step - loss: 0.2011\n",
      "Epoch 68/150\n",
      "30/30 [==============================] - 5s 160ms/step - loss: 0.1927\n",
      "Epoch 69/150\n",
      "30/30 [==============================] - 5s 153ms/step - loss: 0.1859\n",
      "Epoch 70/150\n",
      "30/30 [==============================] - 5s 145ms/step - loss: 0.1791\n",
      "Epoch 71/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.1734\n",
      "Epoch 72/150\n",
      "30/30 [==============================] - 5s 136ms/step - loss: 0.1647\n",
      "Epoch 73/150\n",
      "30/30 [==============================] - 5s 138ms/step - loss: 0.1588\n",
      "Epoch 74/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.1532\n",
      "Epoch 75/150\n",
      "30/30 [==============================] - 5s 136ms/step - loss: 0.1476\n",
      "Epoch 76/150\n",
      "30/30 [==============================] - 5s 147ms/step - loss: 0.1422\n",
      "Epoch 77/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1373\n",
      "Epoch 78/150\n",
      "30/30 [==============================] - 5s 146ms/step - loss: 0.1319\n",
      "Epoch 79/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1272\n",
      "Epoch 80/150\n",
      "30/30 [==============================] - 5s 145ms/step - loss: 0.1230\n",
      "Epoch 81/150\n",
      "30/30 [==============================] - 5s 159ms/step - loss: 0.1186\n",
      "Epoch 82/150\n",
      "30/30 [==============================] - 5s 149ms/step - loss: 0.1152\n",
      "Epoch 83/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1123\n",
      "Epoch 84/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1100\n",
      "Epoch 85/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1074\n",
      "Epoch 86/150\n",
      "30/30 [==============================] - 5s 145ms/step - loss: 0.1054\n",
      "Epoch 87/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1033\n",
      "Epoch 88/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1022\n",
      "Epoch 89/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1009\n",
      "Epoch 90/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1004\n",
      "Epoch 91/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1001\n",
      "Epoch 92/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.1009\n",
      "Epoch 93/150\n",
      "30/30 [==============================] - 5s 154ms/step - loss: 0.1033\n",
      "Epoch 94/150\n",
      "30/30 [==============================] - 5s 147ms/step - loss: 0.1070\n",
      "Epoch 95/150\n",
      "30/30 [==============================] - 5s 154ms/step - loss: 0.1174\n",
      "Epoch 96/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1452\n",
      "Epoch 97/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.2147\n",
      "Epoch 98/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.3040\n",
      "Epoch 99/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.3224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.2750\n",
      "Epoch 101/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.2193\n",
      "Epoch 102/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1743\n",
      "Epoch 103/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1424\n",
      "Epoch 104/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1176\n",
      "Epoch 105/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.1015\n",
      "Epoch 106/150\n",
      "30/30 [==============================] - 5s 151ms/step - loss: 0.0920\n",
      "Epoch 107/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0867\n",
      "Epoch 108/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0834\n",
      "Epoch 109/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0817\n",
      "Epoch 110/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0806\n",
      "Epoch 111/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0795\n",
      "Epoch 112/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0790\n",
      "Epoch 113/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0786\n",
      "Epoch 114/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0780\n",
      "Epoch 115/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0779\n",
      "Epoch 116/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0774\n",
      "Epoch 117/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0773\n",
      "Epoch 118/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0772\n",
      "Epoch 119/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0769\n",
      "Epoch 120/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0769\n",
      "Epoch 121/150\n",
      "30/30 [==============================] - 5s 143ms/step - loss: 0.0769\n",
      "Epoch 122/150\n",
      "30/30 [==============================] - 5s 148ms/step - loss: 0.0767\n",
      "Epoch 123/150\n",
      "30/30 [==============================] - 5s 149ms/step - loss: 0.0767\n",
      "Epoch 124/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.0766\n",
      "Epoch 125/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.0766\n",
      "Epoch 126/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.0768\n",
      "Epoch 127/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.0767\n",
      "Epoch 128/150\n",
      "30/30 [==============================] - 5s 142ms/step - loss: 0.0766\n",
      "Epoch 129/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0767\n",
      "Epoch 130/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0769\n",
      "Epoch 131/150\n",
      "30/30 [==============================] - 5s 139ms/step - loss: 0.0770\n",
      "Epoch 132/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0771\n",
      "Epoch 133/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0774\n",
      "Epoch 134/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0776\n",
      "Epoch 135/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0782\n",
      "Epoch 136/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0789\n",
      "Epoch 137/150\n",
      "30/30 [==============================] - 5s 137ms/step - loss: 0.0807\n",
      "Epoch 138/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.0849\n",
      "Epoch 139/150\n",
      "30/30 [==============================] - 5s 138ms/step - loss: 0.1021\n",
      "Epoch 140/150\n",
      "30/30 [==============================] - 5s 142ms/step - loss: 0.2626\n",
      "Epoch 141/150\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.5988\n",
      "Epoch 142/150\n",
      "30/30 [==============================] - 5s 138ms/step - loss: 0.5618\n",
      "Epoch 143/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.4221\n",
      "Epoch 144/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.3170\n",
      "Epoch 145/150\n",
      "30/30 [==============================] - 5s 158ms/step - loss: 0.2406\n",
      "Epoch 146/150\n",
      "30/30 [==============================] - 5s 140ms/step - loss: 0.1890\n",
      "Epoch 147/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.1533\n",
      "Epoch 148/150\n",
      "30/30 [==============================] - 5s 141ms/step - loss: 0.1265\n",
      "Epoch 149/150\n",
      "30/30 [==============================] - 5s 140ms/step - loss: 0.1075\n",
      "Epoch 150/150\n",
      "30/30 [==============================] - 5s 144ms/step - loss: 0.0945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f324c0f0760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate and train the model\n",
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(dataset, epochs=150)  # start with a small number of epochs to check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c245a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step class\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f53d3d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La felicidad es una cosa que no siempre la guiara. Y uno no puede pasarse toda mujer como si no imposible propósito.\n",
      "- Zygmunt Bauman\n",
      "Dramen dolor, tampoco negaria deabulde a causa de los inversorables. Ya no mira con lo más breve: el presente, sin hobribirse o admirable, pues muchas cosas, pero has fallado en lo esencial, es decir, has fracasado. Espero está presentado en nuestro amor de Cristos y ses injurias, sin pensamientos, la justicia. Comenza el presente, para sí mismo es uno de los que tienen miedo a la oscuridad\n",
      "- Stephen Hawking\n",
      "He estado inmerso es el derecho que nada puede hacer durante ocho horas, día tras día a quienes nos amamos. El amor es demilisar.\n",
      "- Martin Luther King\n",
      "Quizá sueña se alimenta de supersoda en vuestro corazón; una buena actividad cree.\n",
      "- Eurípides\n",
      "Pero la vida no es la que uno vivió, sino la que uno recuerda, y cómo la relativida como cuanto más repedente que la pérdida del paraíco del asceso de su vida es despertar la verdad...\n",
      "- Mario Benedetti\n",
      "En el amor es virtuoso ser fiel mas no fanático\n",
      "- Mario Benedetti\n",
      "El amor no es repetición. Cada acto de amor es un ciclo en sí mismo, una órbita cerrada en su propio ritual. Es, cómo podría explicarte, un puño de vida en el que cambiar demasiado pronto\n",
      "- William Faulkner\n",
      "El presidente Roosevelt un signo de incención en el mundo ni en el ejemplo de la comprensión, de la destrucción de la vida y la humanidad permatera. Por enemor progresivo es el que más ante vusgas o no existe.\n",
      "- Tony Gaskins\n",
      "No puedes solo hablar de tu amor.\n",
      "- José Ortega y Gasset\n",
      "La vida está en tus vidas, fueran por ella es tan importante que es el más grande que la sombra de tu propia vida y otro es el fruto maduro de la vida.\n",
      "- Oscar Wilde\n",
      "La vida es simplemente una huito de nuestra existencia.\n",
      "- Heráclito\n",
      "Inmortales, los mendigos ilustres del cielo como una forma de vida que ha maladora y hoy mientras alterramos.\n",
      "- Gustave Flaubert\n",
      "El amor no es lo primero en la vida, sino de dar vida a los años.\n",
      "- Anónimo\n",
      "Todos llevamos dentro unas posibles vidas, para ser feliz en los mendas mestigacios, la especie singula de entren infinito\n",
      "- Virginia Woolf\n",
      "Alguien tiene que morir para que el resto de nuestra vida nos da al corazón del hombre\n",
      "- Anónimo\n",
      "En la vida hay gran mayor que las reúnencia de los hombres.\n",
      "- Benedicto XVI\n",
      "Dios quiere que ame cuándo la espiritua sucedido por la estral en un cuerpo fueron y no conde el del camino del martirno de mi vida, no moriría nuevo de expresarlos\n",
      "- Carlo Dossi\n",
      "Como todavía soy de impulsar al cielo y renueva, con frecuencia me parece encantador para olvidar cuantos años de tierra. Una esperanza que os dicho,s una vida futura. Me dure hasta la guerra cien íado cuando espera salvar la vida.\n",
      "- Viktor Frankl\n",
      "Nos divinas siempre dejando un tan dado la vida. Que no hay nada pedir ser menos afecto. Eso es lo que ahora se convierte en el sentido tranquiliza\n",
      "- Juan de la Cruz\n",
      "La vida es simple reconocible en la familia, una ley no decepcional. Hay muchos que empiezan por broma y tu cuerpo, toda \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.143800497055054\n"
     ]
    }
   ],
   "source": [
    "# finally use the model to generate text\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['La felicidad es'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(3000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762f11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
